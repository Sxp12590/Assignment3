{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJnhDExRrPa3",
        "outputId": "06d2e81f-1086-4eab-b085-41a4551fb785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text snippet: First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n",
            "Total unique characters: 65\n",
            "Epoch 1/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 53ms/step - loss: 3.1819\n",
            "Epoch 2/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - loss: 2.0610\n",
            "Epoch 3/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 1.7601\n",
            "Epoch 4/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - loss: 1.5918\n",
            "Epoch 5/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - loss: 1.4875\n",
            "Epoch 6/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 1.4191\n",
            "Epoch 7/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 1.3732\n",
            "Epoch 8/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 1.3341\n",
            "Epoch 9/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 1.2978\n",
            "Epoch 10/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 1.2727\n",
            "\n",
            "Generated Text:\n",
            " What am I doing?tymO\n",
            ".Klr$xyxLTIxBzLHg\n",
            "Y3B?UhF!VFYeAVC.CHdHp&XrBw\n",
            "nUj&zQk&CFKJRDEOUQUa&UjAsJ,OhZbsE\n",
            "- lv!QCr:?DONeHTm's tv3.lXoHSP-HjFUbKd$VOALjHNkNov'HAj.a$LG&XMwK3aGTo&ghLj!LgfTAFKEUrxpdA:rhPncV&WRAHLSons A;RltpRxhZqxYUX-YPggh?yx,DPDFgXcHtAWX,;cj&wj\n",
            "pX&txNNfs:o;TMxl?PBDKj'$T\n",
            "jdHWhisd-jub3V:EILIgrR!l.yWAh\n",
            ";XzstWkWG?uwbF.G3!sEUxSD.HAEyZi?AGSrLlf-?MCCfD e-u'onNdyNZQgO .Sqz,nglYoIIoiU-Z3Md$3\n",
            "B&!?3aR?NpO:VlbGkaBbKUr!hUKUUb,.IY-QAH\n",
            "ArLH?f;C'Nb$Zft!wNnPLhiUSoIdrV3Q3IDufIoH,Ym yJUQiyXzhNYVVJEc;XMggqy?&Jh!Kzf\n",
            "IY WojU&\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. Load a text dataset (using Shakespeare's sonnets as an example)\n",
        "path_to_file = tf.keras.utils.get_file('sonnets.txt', 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
        "text = open(path_to_file, 'r').read()\n",
        "\n",
        "# Print a snippet of the text\n",
        "print(\"Text snippet:\", text[:500])\n",
        "\n",
        "# 2. Convert text to a sequence of characters (one-hot encoding)\n",
        "# Create a list of unique characters\n",
        "chars = sorted(set(text))\n",
        "print(f\"Total unique characters: {len(chars)}\")\n",
        "\n",
        "# Create mappings from character to integer and vice versa\n",
        "char_to_index = {char: index for index, char in enumerate(chars)}\n",
        "index_to_char = {index: char for index, char in enumerate(chars)}\n",
        "\n",
        "# Convert the entire text to integer indices\n",
        "text_as_int = np.array([char_to_index[char] for char in text])\n",
        "\n",
        "# Parameters for the model\n",
        "SEQ_LENGTH = 100  # Length of the sequence\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EPOCHS = 10\n",
        "CHUNK_SIZE = len(text_as_int) // SEQ_LENGTH\n",
        "\n",
        "# Create sequences and targets\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(0, len(text_as_int) - SEQ_LENGTH, SEQ_LENGTH):\n",
        "    seq_in = text_as_int[i:i+SEQ_LENGTH]\n",
        "    seq_out = text_as_int[i+1:i+SEQ_LENGTH+1]\n",
        "    sequences.append(seq_in)\n",
        "    targets.append(seq_out)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)\n",
        "\n",
        "# 3. Define the RNN model using LSTM (Non-stateful)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(chars), 256, input_length=SEQ_LENGTH),\n",
        "    tf.keras.layers.LSTM(512, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(512, return_sequences=True),\n",
        "    tf.keras.layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# 4. Train the model\n",
        "model.fit(sequences, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 5. Generate new text by sampling characters one at a time (without reset_states)\n",
        "def generate_text(model, start_string, temperature=1.0, num_generate=500):\n",
        "    input_eval = [char_to_index[char] for char in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)  # Add batch dimension\n",
        "\n",
        "    # Empty list to hold the generated text\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :]  # Get the last character predictions\n",
        "\n",
        "        # Scale the logits by temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Convert the predicted character to text\n",
        "        generated_text += index_to_char[predicted_id]\n",
        "\n",
        "        # Use the predicted character as the next input\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate new text using a seed string\n",
        "seed_string = \"What am I doing?\"\n",
        "generated_text = generate_text(model, seed_string, temperature=0.7)\n",
        "print(\"\\nGenerated Text:\\n\", generated_text)\n"
      ]
    }
  ]
}